<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gele on syx的日志</title>
    <link>cartershi.github.io/blog/tags/gele/</link>
    <description>Recent content in gele on syx的日志</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Jul 2019 13:24:42 +0800</lastBuildDate>
    
	<atom:link href="cartershi.github.io/blog/tags/gele/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chapter20</title>
      <link>cartershi.github.io/blog/post/hdsa_chapter20/</link>
      <pubDate>Wed, 03 Jul 2019 13:24:42 +0800</pubDate>
      
      <guid>cartershi.github.io/blog/post/hdsa_chapter20/</guid>
      <description> Quadtrees and Octrees quadtree本意是将一个二维区域用平行于坐标轴的两条线将其分为4个区域作为树的4个孩子，因此称为quad tree。在3维空间，则有$2^3=8$个孩子，这叫做octree。更高维度的被称为hyperoctrees。通常情况下quadtree，octree，hyperoctree都叫quadtree。kd-tree与quadtree的不同在于kd-tree每层只选择某一维作为拆分的标准，所以只有两个孩子。竟然有专门的书介绍quadtree的相关内容，可见它多么复杂。
Point Data Point Quadtree 多维点是最简单的情况，通常对于它有这样几种查询：1.区间查询，19章range tree处理的问题。2.球型区域查询，给定点和半径，查这个球里面的区域。3.查找每个点的最近邻。当然查找、插入、删除这些基本操作也是要支持的。
考虑二维情况，point quadtree就是在当前区域选一个点作为根，用它将区域分成四部分，每部分是一颗子树（如果按x轴分成两个区域，就是一颗BST）。
建立：将n个点直接排序，取中间节点，$O(n)$时间将其分成4个子区间，这样保证每个子区间不会超过n，用数学归纳法得建立时间为$O(n\log n)$。
查询：类似BST的查找，不过要比较两维。
插入：先查找到叶子，然后在叶子下面挂1层高树。
删除：很麻烦，书上没写。
在d维情况下将会导致每个点有$2^d$个孩子，kd-tree可以处理这个问题，因为它轮流的选取d维作为每一次层的标准，这样就相当于编码了$2^d$个孩子，可以在不平衡的地方进行处理，完全不处理的kd-tree等价于一棵quadtree。接下来只关注 region quadtree，即平分区间长度而非平分所有点。这样不能向point quadtree保证产生一棵平衡树，但是作者喜欢。
Region Quadtree 定义cell类似于树中node，subcell相当于后代，supersell为祖先，immediate描述父子关系。空间分析非常显然，point quadtree提到四个操作都很容易处理，注意插入时需要不断地细分空间直至两点被分开，删除可以需要不断的往根删直至有至少两个孩子。
Compressed Quadtrees and Octrees Quadtree 鸽了
Spatial Queries with Region Quadtrees Range Query </description>
    </item>
    
    <item>
      <title>Chapter11</title>
      <link>cartershi.github.io/blog/post/hdsa_chapter11/</link>
      <pubDate>Tue, 02 Jul 2019 19:08:12 +0800</pubDate>
      
      <guid>cartershi.github.io/blog/post/hdsa_chapter11/</guid>
      <description>Balanced binary search trees 直接跳到11.3节了，平衡树平衡策略分为三种：子树高度的限制，子树大小的限制，多路搜索树的二进制化。
重新平衡的思路基本上都是从叶子开始往根修复平衡，修复的方法是rotation，它将一个子树的高度减1，另一个加1。大多数的算法逆着路径上执行一次单旋或双旋。另外一种重新平衡的思路是重建某一颗子树。
有一些BST的重平衡时间低于log，这在一些场合比较有效，例如计算几何中有些情况下rotation复杂度不是O(1)。以及并发的重平衡。
AVL Tree AVL树是1962年发明的远古数据结构，它基于子树的高度进行平衡，采用rotation重平衡。AVL树要求对于任意点，它的子树高度差最多为1。直接对高度做induction，可得高度为h的AVL树至少有$F_{h+2}-1$个点，即深度是$O(\log n)$。重平衡是逆着路径进行单旋，或者双旋，或者调整子树信息。插入时最多进行一次旋转。在AVL树中如果只插入或者只删除，那么均摊时间是$O(1)$，否则是$O(\log n)$。
Weight-balanced tree Weight-balanced树基于子树的weight进行平衡，weight定义为叶子节点数。它要求每个点的weight与它右孩子的weight的比值在$[\alpha,1-\alpha]$之间。每上升一层，weight至少为原来的$\frac{1}{1-\alpha}$，这样得到深度和weight之间的关系。重平衡也是单旋或双旋，不同的是重平衡完成后再次重平衡至少需要当前weight的常数倍次的更新，这就导致了所谓的均摊时间少于$O(\log n)$。
Balanced Binary Trees Based on Multi-Way Trees binary B-trees类似trie的二进制实现那样，把一个节点拆成多个节点实现成二叉树。Symmetric Binary Trees或者通常被称为红黑树是2,3,4树的二进制实现。AA trees与红黑树等价，但是实现起来很容易。完全不讲细节的
看到11.5</description>
    </item>
    
    <item>
      <title>chapter14</title>
      <link>cartershi.github.io/blog/post/hdsa_chapter14/</link>
      <pubDate>Mon, 24 Jun 2019 12:06:55 +0800</pubDate>
      
      <guid>cartershi.github.io/blog/post/hdsa_chapter14/</guid>
      <description>Randomized Dictionary Structures 引言 worst-case高效的数据结构太复杂并且写起来很麻烦，所以人们试图优化一串操作的总代价，而非单次操作的代价，这就是所谓的均摊复杂度。从随机的角度，目的是限制操作的期望并是的超出阈值的概率不大。
为了简化分析，本章仅考虑支持插入、删除、搜索操作的字典，字典集为有限整数，并且元素之间两两不相同。
本章讨论的随机算法是Las Vegas algorithm，即只要返回结果，结果就是对的。另外一种是Monte Carlo algorithm，运行时间固定，但是结果不一定对。
$1-O(\beta^{-n}),1-O(n^{-c})$都是high confidence bound，$1-O((\log n)^c)$是very good confidence bound，$1-O(\beta^{-\alpha})$是constant probability bound。对于本章，通常考虑运行时间与期望时间的bound。
然后是一堆概率论的基础知识。
Skip Lists skip list的思路是当将有序链表$S_0$中随机一些节点复制进入$S_1$，接着从$S_1$复制一些节点到$S_2$，直至顶层为空。即前一层的节点以某个概率$p$继续成为上一层的节点，并将这两个节点连接起来，同层的节点也要连接起来。所以每个节点是一个变长的数组，记录每层的内容。如下图所示，32是一个节点，它有6层，每层之间相互连接，这样方便往右下搜索。
直接计算每一数的高度大于l的概率，然后直接算期望得高度为$O(\log n)$，随便union bound一下即可证明W.H.P.
空间复杂度期望$O(n)$，W.H.P.就Chernoff bound一下n个随机变量层高得空间复杂度$O(n)$。
查找：从顶层开始，尽可能的往右走，保证当前节点的值不大于目标节点，走到不能走则前往下一层的同列节点。
插入：按照查找的过程查找位置，不同在于需要记录每层最后一个不大于目标节点的节点。然后随机生成这个节点的层数，将其连在查找在记录下的节点后。
删除：类似查找并尝试在每层中删除即可。注意可能产生的空行。
以上三个操作运行时间都是由查找时间决定的。我们逆向考虑查找的过程，从底层0到顶层r的过程中每次进入上层时这个值都是上层中最接近x的值，即每次都是以$p$的概率往上走，以$1-p$的概率往左走，这等价于r个几何随机变量$Y_i$，每个表示第$i$层往左走的步数。注意到第$r$层为空，所以总的搜索步数为$Y=r+\sum_{i=0}^{r-1}Y_i$。这个转化也太帅了！
令$H_r=\sum_{i=0}^{r-1}Y_i$，由$P[r &amp;gt; 4\log n]&amp;lt;\frac{1}{n^3}$，$P[H_r&amp;gt;16\log n]&amp;lt;\frac{1}{n^2}\ for\ n\ge 32$是先全概率公式展开，再用Chernoff bound，则$P[H_r&amp;gt;16\log n\cup r &amp;gt; 4\log n]&amp;lt;\frac{1}{n^2}+\frac{1}{n^3}=O(\frac{1}{n^2})$，即$P[Y=O(\log n)]&amp;gt;1-O(\frac{1}{n^2})$。对于每一个x，$P[Y\in O(\log n)]&amp;gt;1-O(\frac{1}{n^2})$，所以n+1种的路径均为$O(\log n)$的概率至少为$1-O(\frac{1}{n})$，这就证明了三个操作W.H.P都是$O(\log n)$。
Randomized Binary Search Trees Randomly Built Binary Tree(RBBT)指只有随机插入，即给定序列的树，插入在不同间隙里的概率是相等的。这个假定完全不符合实际，并且删除操作会使得RBBT退化严重，高度达到$\sqrt n$。
RBST和treap不同，它不随机产生第二有限值，而是保存当前节点的子树点数，并维护这样的性质$P[size(L)=i|size(T)=n]=\frac{1}{n}$，即不同大小左子树的概率相等，相当于有序列表里每个元素等概率为根。
以下建议全看原论文，好神难啊QAQ
插入：当插入一个n个点树时，随机一个0..n的数，若不是n，进入相对于的子树继续插入，否则x应当为根，递归的产生$T&amp;lt;,T&amp;gt;$，不妨设x小于根，根r及其右孩子因设为$T&amp;gt;$，递归调用$r.L,x,T&amp;lt;,T&amp;gt;.L$，这是指$T&amp;lt;$需要继续添加，$T&amp;gt;$已经填好了根和右子树，只需要填左子树。
删除：先正常的搜索到这个节点，然后将这个节点的左子树和右子树join起来，join是判断随机产生的数[1,size(T)-1]是在左子树还是右子树，若在左子树，则将左孩子的右子树与右子树join，否则将右孩子的左子树与左子树join。
插入的复杂度与普通插入一样，用递归证明产生的两个子树也是RBST，同时$T&amp;gt;.R$也是RBST，则证明$T&amp;gt;$是RBST只需要证明根也满足条件，注意这个根是原树的根，所以用条件概率即可的证明。
现在考虑插入的结果，若未插入在根，则第一次会使概率变为$\frac{1}{n+1}$，递归得到一个RBST，原树剩下的部分也是RBST。否则用上面的结论另外注意x也是$\frac{1}{n+1}$的概率。
对于join，递归证明join后的子树是RBST，左孩子的左子树是RBST，右子树更小的join结果，也是RBST，互相独立，每个元素为根的概率是$\frac{1}{n+m}$。</description>
    </item>
    
    <item>
      <title>Chapter13</title>
      <link>cartershi.github.io/blog/post/hdsa_chapter13/</link>
      <pubDate>Mon, 24 Jun 2019 00:15:52 +0800</pubDate>
      
      <guid>cartershi.github.io/blog/post/hdsa_chapter13/</guid>
      <description> Splay trees 对于m个操作的序列，splay运行时间为$O(m\log n)$，所以均摊时间为$O(n)$。可以用来处理linking cutting trees，并且rebalance操作比平衡树简单很多。又是tarjan
复杂度分析 zig、zag是单旋（最多发生一次），zigzig、zagzag是父亲及父亲的父亲在同侧，zigzag、zagzig是父亲及父亲的父亲在异侧，直至将x转到根，这个操作称为splay(x)，这个操作均摊复杂度为$O(log n)$。
w(x)是x的权重，size(x)是以x为根的子树的点权重和，rank(x)是$\log_2(size(x))$，Potential(T)=$\alpha\sum_{x\in T}rand(x)$。均摊时间=实际时间+新Potential-老Potential。splay的运行时间为$\beta\times$Number of rotations。其中$w(x)\ge 0,\ \alpha\ge\beta&amp;gt;0$为任意给定值。
将splay的旋转按上面分的三种旋转进行逐个分析，累加得，将对根为t的树进行splay(x)的均摊时间为$3\alpha(rank(t)-rank(x))+\beta=O(\log\frac{size(t)}{size(x)})$。
令$w(x)=\frac{1}{n}$，$\alpha=\beta=1$，则单次splay的均摊时间为$3\log n+1$，Potential的总变化最多为$O(n\log n)$，则进行m次splay操作的均摊时间是$O(3m\log n + m)$，得实际时间为$O(3m\log n + m)+n\log n=O((m+n)\log n)$。
以下这几个操作的定义与12章treap中的定义完全一致。
查找：按照BST查找，若成功找到，则对找到的点进行一次splay，否则对发现失败的点的父亲进行splay。
插入：用BST的查找找到叶子节点，未找到则对叶子splay，否则插在这个叶子的子树上，然后对其splay（这里与论文中的插入不同）。
合并：在t1里查找正无穷，则最大的点变成了t1的根，让t2变成t1的右子树。
分裂：调用查找x的过程，判断根与x的大小关系分开根和它的左或右子树。
删除：调用查找的过程，若找到，则因为查找最后调用了splay，所以根为待删除的节点，将根的左右子树进行合并即可。
书中的分析缺少这是势能定义，还是按论文里来。对于m个操作的序列，定义Potential所有树的Potential的和操作前不在树中的数的w和，然后对每种操作都类似splay可以分析出一个均摊的时间（基本上是splay的均摊时间+不同操作导致的势能变化），Potential的总变化最多为$O(n\log n)$，将它们累加得总时间为$O((m+n)\log n)$。
静态最优 上面的分析只是基于$w(x)=\frac{1}{n}$得出的，若定义$q(i)$为i被查询的次数（直接或间接），定义$w(x)=\frac{q(i)}{\sum q(i)}$，得splay均摊时间$-3rank(x)+1=O(1+\log\frac{\sum q(x)}{q(x)})$，求和得$O(\sum q(x)+\sum q(x)\log\frac{\sum q(x)}{q(x)})$，Potential最大变化为$O(\sum\log\frac{\sum q(x)}{q(x)})$，由于$m=\sum q(x)$，所以总时间为$O(m+\sum q(x)\log\frac{m}{q(x)})$，这是二叉搜索树的最优结果。类似的思路可以处理finger search，可证明它比finger search tree更好。
working set property：若只有t个元素被查询，查询的时间是$O(\log t)$，若$i$的两次访问之间间隔了$t$个不同的数，则i的第二次查询时间为$O(\log(t+1))$，这里主要用到了一个权重重排的思想，首先给定一个权重，第i个首次出现元素权重设为$\frac{1}{i^2}$，当某个元素被查询时，将它的权重变为1，比他大的权重都相应的向后挪一位（相当于循环调整），这样的重拍只会Potential变小，最终时间=均摊时间+老Potential-新Potential，得类似上面的分析算出的时间只会比真实之间更大。这也太狠了
附一篇文章Alternatives to splay trees with O(log n) worst-case access times,John Iacono
splay由于w函数可以具体设定，所以它被推测可能是动态最优的，即对于任意的序列，它都可以和最好的动态二叉搜索树一样。以被证明splay树有类似finger search的性质，搜索接近的目标很快。
LCT </description>
    </item>
    
    <item>
      <title>Chapter12</title>
      <link>cartershi.github.io/blog/post/hdsa_chapter12/</link>
      <pubDate>Sat, 22 Jun 2019 19:23:11 +0800</pubDate>
      
      <guid>cartershi.github.io/blog/post/hdsa_chapter12/</guid>
      <description>Finger Search Tree Finger Search指从finger $x$出发寻找$y$。比如有序数组，从$x$开始倍增，这里不能acm里的那种倍减trick，因为那样是$O(\log n)$的，而倍增是$O(\log d)$的。$d$是$x$和$y$在有序数组中的排名差。如下图所示，意识流一下就是那样了。
Dynamic Finger Search Trees 上面的倍增已经解决了这个问题，但是有序数组不能高效插入删除，所以需要Dynamic Finger Search Tree。
Level Linked (2,4)-Trees B树它leile，2-4个孩子的B树，基于finger的插入删除均摊复杂度为$O(1)$，期望倒是会，均摊不会分析。它与B树的不同在于所有的边都是双向的，并且同层相邻节点之间边相连。
不妨令$x&amp;lt;y$，走到父亲$v$，同时判断$v$的右邻居$v&amp;rsquo;$是否是$y$的父亲。当停止时：
若因为停在共同的祖先$c$，则$c$至少有一个子树隔开$x,y$，这颗子树里的点都在$x,y$之间，它的点数就bound住了高度。
若是因为$v$的右邻居$u&amp;rsquo;$是$y$的父亲，此时$x$在$v$的右子树，$y$在$v$的父亲的右邻居$u&amp;rsquo;$的左子树，只需要考虑$v$和$u&amp;rsquo;$的子树的末端相邻点，则变成第一种情况。Q.E.D.
综上，用(2,4)-Tree的时间复杂度是$O(\log d)$的。
Randomized Finger Search Trees treap细节完全不介绍的吗？背景知识参考这里。
treap treap的思路就是一个二叉搜索树，但是它不像平衡树那样根据树高左旋右旋，而是根据随机产生的第二优先值维护一个堆（下面只考虑小根堆）。比如当前子树根为A，左子树根为B，右子树为C，不妨B的第二优先值最小，则B右旋成根，它的右子树变成A的左子树。单旋谁不会啊
下图字母是键值，数字是第二优先值。
插入：首先按二叉搜索树将其真实值插入到叶子，然后就堆插入元素那样按第二优先值从最底层往上调，调整就是用的左旋右旋。
删除：先找到这个节点，然后将其旋转到叶子，最后删除掉，逆着插入的操作
分裂：即分裂成比某个值$\pi$大的和小的两个treap。插入$\pi$，令它的第二优先值为无穷小，那么插入完它为根，且由二叉搜索树得，它的两个子树是要求的treap。
合并：用$\pi$作为根合并两棵treap，然后将根用删除的操作删除。
以上4个操作的复杂度均由treap的高度决定，可证明随机情况下高度为$O(\log n)$。
引理：将元素按键值递增为数组A，在treap中第i小的元素为第k小元素的祖先等价于i的第二优先值是A[i..k]中最小的(i&amp;lt;k)。在treap中第i小的元素为第j小元素的祖先等价于i的第二优先值是A[j..i]中最小的(j&amp;lt;i)
证：
⇒：考虑当前treap的根是谁。若是i，则得证；若是k，则矛盾；若在区间(i,k)，则i和k不在同一子树，矛盾；若不在区间[i,k]，则是更小的treap，递归得证。
⇐：依然分类讨论根。Q.E.D.
这个引理在下面的证明中会被直接使用，就不明说了。
定理1：以上四个操作期望复杂度都是$O(\log n)$的。
证：考虑每个元素深度的期望，即祖先个数的期望，即每个点是其祖先的概率，等价于每个区间中它是最小的，将区间拆成以它结尾的和以它开头的分开计算，两个都是$\sum\frac{1}{i}=O(\log n)$级别。所以以上操作都是$O(\log n)$的。
定理2：证明树高$O(\log d)$是W.H.P.的。
证：W.H.P.通常很多是用chernoff bound来证明的，类似树高的期望证明，将区间拆成两个，每个补充到n个随机变量，每个直接用chernoff bound即可。Q.E.D.
接下来分析基本来自96年Raimund Seidel的那篇Randomized Search Trees。或者算法导论红黑树那章最后一道作业题。
定理3：给定finger下，插入删除复杂度为$O(1)$。
证：定义点v的left spine为从v开始只走左边的最长路径，right spine为从v开始只走右边的最长路径。
对于待删除的点x，定义$SL(x)$为x的左孩子的right spine，$SR(x)$为x的右孩子的left spine。虽然很别扭，但是这个定义很符合单旋的性质：若x左旋，即x变成左孩子，x的右孩子的左子树变为x的右子树，则$SL(x)$不变，$SR(x)-1$。若右旋，即x变为右孩子，x的左孩子的右子树变为x的左子树，则$SL(x)-1$，$SR(x)$不变，所以每次旋转$SL(x)+SR(L)$减少一。左、右两个字我已经不认识了。
注意到$SL(x)$显然是$x$到$x-1$的路径（这里所有的$x$都是指A数组中第$x$个元素），若$x-1$不在$x$的子树中，则$x$没有左孩子了，否则$x-1$一定在right spine的终点。所以期望意义下$SL(x)$为A[1&amp;hellip;x-1]中所有$x-1$的祖先减去$x$的祖先，即$SL(x)=\sum_{i=1}^{x-1}\frac{1}{(x-i+1)(x-i)}=1-\frac{1}{x}$。同理$SR(x)=1-\frac{1}{n+1-x}$。
也就是说给定finger的情况下，$O(1)$时间可以删除它，由于插入为删除的逆过程，给定finger时也是$O(1)$时间插入。Q.E.D.
定理4：接下来证明$x$到$y$的路径长度期望是$O(\log d)$。</description>
    </item>
    
  </channel>
</rss>