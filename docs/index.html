<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.55.6" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>syx的日志</title>

  
  <link type="text/css" rel="stylesheet" href="cartershi.github.io/blog/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="cartershi.github.io/blog/css/poole.css">
  <link type="text/css" rel="stylesheet" href="cartershi.github.io/blog/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="cartershi.github.io/blog/css/hyde.css">  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">

  <script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="cartershi.github.io/blog/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="cartershi.github.io/blog/cai.jpg">

  
  <link href="cartershi.github.io/blog/index.xml" rel="alternate" type="application/rss+xml" title="syx的日志" />
  
  
</head>

  <body class="theme-base-08 ">
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <a href="cartershi.github.io/blog/"><h1>syx的日志</h1></a>
      <p class="lead">
       洗澡王 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="cartershi.github.io/blog/">Home</a> </li>
        <li><a href="cartershi.github.io/blog/post/"> Posts </a></li><li><a href="cartershi.github.io/blog/about/"> About </a></li><li><a href="cartershi.github.io/blog/tags/"> Tags </a></li>
      </ul>
    </nav>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/a-tight-bound-on-approximating-arbitrary-metrics-by-tree-metrics/">A tight bound on approximating arbitrary metrics by tree metrics</a>
  </h1>
  <time datetime="2019-07-09T20:57:35&#43;0800" class="post-date">Tue, Jul 9, 2019</time>
  作者：Jittat FakcharoenpholSatish Rao and Kunal Talwar
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/a-greedy-approximation-algorithm-for-the-group-steiner-problem/">A greedy approximation algorithm for the group Steiner problem</a>
  </h1>
  <time datetime="2019-07-08T09:39:22&#43;0800" class="post-date">Mon, Jul 8, 2019</time>
  作者 Chandra Chekuri, Guy Even, Guy Kortsarz
2. Preliminaries 本文处理的是group steiner tree problem restricted to trees，输入为一个有根的非负权有向树T以及m个点集$gi$，树T&rsquo;被称为z-cover表示它至少了与z个点集相交，目标是找到一个权重最小的z-cover树。记$n=|\cup{i=1}^{m}gi|,s=\sum{i=1}^{m}|g_i|$，点集$g_i$中的点都称为terminal（终端）。
虽然这里处理的图是树，但是A tight bound on approximating arbitrary metrics by tree metrics(STOC&rsquo;03)证明了用树可以$O(\log n)$近似图中两点距离，所以可以直接用那个近似图之后用本文对得到的树来近似。
预处理：注意到非终端的叶子可以直接删除，树中度为2的非终端节点也可删除，再用一个集合包含树中所有的点，这样新树的点数是$O(n)$，且每个点都是终端，最后scale边权使得所有的正边权都大于等于1。
$\alpha-faithful$树的定义看起来就是为了claim2.3准备的。
3. A recursive greedy algorithm with geometric search 引用的算法，思路在HAAM5里面有过介绍（算法不同，证明的思路类似），就是对于一棵树，首先使用height reduction将其高度减少为$O(\frac{\log n}{\log\log n})$，这会导致结果近似，然后用degree reduction将每个点的度减少为$O(\log n)$，这样是精确的。在这棵树上做类似集合覆盖的算法，近似比直接套用那里的引理。
Geometric search主要的不同在于不穷举所有的子集，而是取幂次，这招height reduction也用了，不过这里不从零开始。
lemma3.3证明用了geometric之后就可以把时间复杂度减少到n的多项式，与树高无关，它的证明思路也就是bound调用子函数的次数，每一轮的调用次数容易bound，循环的轮数要注意子函数最少返回覆盖z&rdquo;/h个终端的树，z&rdquo;又有下届，所以可以bound。
lemma3.4的证明和引用的那个很像，不同在于两点：1.有一些重要的点和不重要的点，不重要的点由于geometric不会被调用，直接近似掉。2.考虑重要的点中密度最小的子树，它对应的参数由于geometric不一定调用，但是调用最靠近它的参数的那次与它之间密度的关系。剩下的证明就一样了。
4. Height reducing transformation 这玩意不是一个HAAM第5章里面的height reduction的，那里处理的是完全图，这里处理的是树。
$\alpha-decomposition$就是对$T_r$dfs，遇到轻节点回溯，这样访问到的所有节点。skeleton是$\alpha-decomposition$中的$T_r$的重节点形成的子树。branch定义我看了好久，它误导人的地方在于maximal subpath，其实就是从不止一个孩子的节点往下走，直至碰到一个多孩子的节点，这就是一个branch。（个人觉得它这里的branch包含了skeleton的叶子到分支节点的路径，不然没有$2\alpha-1$个branches吧）搞懂这个定义，算法就很容易理解了，首先得到$\alpha-decomposition$，对它的skeleton进行dfs计算所有的branch，计算所有branch的bunch，用3层高的路近似根到它的路径。
高度直接递归计算。近似比的证明思路在于原树的$\alpha-decomposition$和reduce完的那个三层的树一一对应，考虑每个$\alpha-decomposition$的每个branch到根的联通树$Q[S_B]$，它与对应的$Q&rsquo;[S_B&rsquo;]$可用bunch证明近似，在由于branch两两不相交且个数为$O(\alpha)$，所以$\sum Q&rsquo;$可用$O(\alpha)Q$bound这其实也可以直接分析每个branch，而不带根，然后bound边被重算的次数，就是HAAM5章那个思路。
5. Degree reducing transformation 这个算法就是将树转化成一颗每个点度最多$\beta$的树，使得树的高度不会增加超过$\log_{\beta/2}n$。这个结论感觉挺有用的。
算法就是将小的节点合并。用一个点拼接它。
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/haam_chapter5/">HAAM_chapter5</a>
  </h1>
  <time datetime="2019-07-07T12:13:42&#43;0800" class="post-date">Sun, Jul 7, 2019</time>
  5.2 对于集合覆盖那个著名的贪心，如果每次并不选择密度最小的集合$Si$，而是对于剩下x个未选择的元素，一个oracle可以给出最小密度$\alpha(x)$近似的，那么$\frac{\sum w(Si)}{w(C*)}\le\int{n-|\cup{S&rsquo;\in C_i}S&rsquo;|}^n{\frac{\alpha(x)}{x}dx}$，这个结论的证明不是很难，需要理解的地方在于递归证明里第一个不等式是需要进行两次放缩的。而(5.2)的递归公式也是隐藏了一个放缩在里面。
这个结论有意思的地方是只要挑选不超过$1-\frac{1}{e}$倍的元素，贪心得到的集合权重不会超过$w(C*)$。
5.3 注意这里DST的定义里输入是给定根r的，DST显然是k-DST的一种情况，k-DST规约到n-shallow k-DST，每层都是原图的点，两层之间的边是原图的边，加自己到自己的零边。
对于k-DST问题，整体的思路是：k-DST等价于原图G的传递闭包TC(G)的k-DST；接着在TC(G)上找l-shallow k-DST，由lemma5.2，它的最优解是k-DST的近似；由lemma5.1，TC(G)上的l-shallow k-DST等价于l层有向无环图上的k-DST；5.4节给出了有向无环图上的k-DST的近似算法。最终用两个等价问题的两个近似解决了一般图上k-DST问题。光这样还没完，这个算法最牛叉的地方在于它的近似比直接依赖于有向无环图上的k-DST的近似算法的过程，即那个类似集合覆盖的过程直接这么复杂的过程书上为什么不讲清楚呢
5.4 这里最难理解的地方是aug树的权重计算了在之前可能已经被加入树中的边，其实这无所谓，我们得到的树的权重不计算重边，所以不会大于aug树的权重和。5.2节的结论说的就是集合权重的和，即得到的树的权重与最优树的权重依然满足这样的结论。（直接思考5.2的证明过程即可）。
算法其实很简单，但是claim5.2的证明非常妙：最优树的参数k&rsquo;也必然k&rsquo;-DST调用，那次调用中累计aug树的过程其中有一个树满足那个条件，记它包含k&rdquo;的目标，则k&rdquo;-DST调用时必然产生这棵树作为候选（由贪心决定），所以aug树的密度不会超过它。这个树构造还用了一个技巧：在第一次超越时，即$k&rdquo;\ge\frac{k&rsquo;}{l-1}$，这个超越虽然会使得分析变得麻烦，但是保证了$k&rdquo;&gt;0$且存在。分析的思路是考虑发生超越的之前的每个时刻的情况，总的最优解在当前的密度大于等于当前最优密度，用当前最优密度的递归式进行放缩，再对总的最优解在当前的密度进行放缩即可。
在分层图上，运行时间的分析只需要注意每个点在构造的图上有n个孩子，每个孩子调用k次k-DST。近似比的分析直接用集合覆盖。
对于一般图上k-DST，过程麻烦一点，需要考虑直接在原图上集合覆盖，这样的一棵树包含k*个目标，则它其实是K*-DST的最优解。所以在分层图上有一个近似的最优解，然后用分层图上k-DST的近似算法得到近似最优解的近似解，这就可以直接用5.2节的结论了。实际上就是分层图上k-DST的近似算法的解是原图上的最小剩余密度树的近似，所以可解。
在GST上，只考虑树上的k-GST，依然是不断的求集合覆盖，不同在于树可以容易的改变高度和度数。接着使用geometric search的思路bound调用子树k-GST的次数，可以得到一个与原树高无关的多项式算法。细节参考写的&rdquo;A greedy approximation algorithm for the group Steiner problem“博客。
Appendix 引理的证明思路是对最优的k-DST进行height reduction，即将极小的重节点连到根上，对极大的轻节点进行递归reduction。考虑根开始的第一次reduction，被连到根上的节点最多$\alpha$个，所以树最多被重复$\alpha$倍，重点在于被复制的路径必然是重节点的父亲，所以它不会被再次复制，所以每次只有原来树上的路径被复制，而高度可以用直接bound。
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95%E6%89%8B%E5%86%8C/">近似算法手册</a>
  </h1>
  <time datetime="2019-07-07T12:08:25&#43;0800" class="post-date">Sun, Jul 7, 2019</time>
  再开一个新坑Handbook of Approximation Algorithms and Metaheuristics，简称HAAM，在这里。这本书的证明实在跳跃，难以看懂，还是记下来好。
如有侵权，请联系删除。
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter20/">Chapter20</a>
  </h1>
  <time datetime="2019-07-03T13:24:42&#43;0800" class="post-date">Wed, Jul 3, 2019</time>
   Quadtrees and Octrees quadtree本意是将一个二维区域用平行于坐标轴的两条线将其分为4个区域作为树的4个孩子，因此称为quad tree。在3维空间，则有$2^3=8$个孩子，这叫做octree。更高维度的被称为hyperoctrees。通常情况下quadtree，octree，hyperoctree都叫quadtree。kd-tree与quadtree的不同在于kd-tree每层只选择某一维作为拆分的标准，所以只有两个孩子。竟然有专门的书介绍quadtree的相关内容，可见它多么复杂。
Point Data Point Quadtree 多维点是最简单的情况，通常对于它有这样几种查询：1.区间查询，19章range tree处理的问题。2.球型区域查询，给定点和半径，查这个球里面的区域。3.查找每个点的最近邻。当然查找、插入、删除这些基本操作也是要支持的。
考虑二维情况，point quadtree就是在当前区域选一个点作为根，用它将区域分成四部分，每部分是一颗子树（如果按x轴分成两个区域，就是一颗BST）。
建立：将n个点直接排序，取中间节点，$O(n)$时间将其分成4个子区间，这样保证每个子区间不会超过n，用数学归纳法得建立时间为$O(n\log n)$。
查询：类似BST的查找，不过要比较两维。
插入：先查找到叶子，然后在叶子下面挂1层高树。
删除：很麻烦，书上没写。
在d维情况下将会导致每个点有$2^d$个孩子，kd-tree可以处理这个问题，因为它轮流的选取d维作为每一次层的标准，这样就相当于编码了$2^d$个孩子，可以在不平衡的地方进行处理，完全不处理的kd-tree等价于一棵quadtree。接下来只关注 region quadtree，即平分区间长度而非平分所有点。这样不能向point quadtree保证产生一棵平衡树，但是作者喜欢。
Region Quadtree 定义cell类似于树中node，subcell相当于后代，supersell为祖先，immediate描述父子关系。空间分析非常显然，point quadtree提到四个操作都很容易处理，注意插入时需要不断地细分空间直至两点被分开，删除可以需要不断的往根删直至有至少两个孩子。
Compressed Quadtrees and Octrees Quadtree 鸽了
Spatial Queries with Region Quadtrees Range Query 
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter19/">Chapter19</a>
  </h1>
  <time datetime="2019-07-02T19:10:04&#43;0800" class="post-date">Tue, Jul 2, 2019</time>
  Interval, Segment, Range, and Priority Search Trees 本章数据结构一般要求数据是静态的，主要关注各种区间搜索问题。
Interval Trees 对于问题：找出给定的S个区间中与给定区间Q相交的区间集合F。
可以建立一颗Interval Tree，当前树的根，它的根v保存左右两个孩子，左子树包含小于v.key的区间，右子树包含大于v.key的区间。v还包含v.key，为S个区间的2S个端点的中位数，v.aux保存包含v.key的所有区间的左右端点，左端点递增，右端点递减。
Interval Tree每次查询的复杂度为$O(\log S+|F|)$：对于当前的根v，若v.key在Q里，则v.aux都放入F，查询两个子树；若v.key&lt;Q.L里，则考察v.aux里的右端点，查询右子树，否则反过来。
Segment Trees Maximum Clique Size of a set of Intervals 问题：对于S个区间的集合，找到最大的一个子集合，使得之中的区间的交非空。
线段树定义不提了，需要注意的是它这里的叶子节点是S个区间的2S个端点从小到大排序，每次插入一个区间，从根开始类似线段树的查询，在每个停止的节点标记的#加1，并用它尝试，并在递归结束时更新clq=max{left.#+left.clq,right.#+right.clq}，clq代表当前节点的子区间目前已知的最大被覆盖数，答案为root.#+root.clq。正确性直接考虑最优解的clq的构成路径，它在路径中的每一个点的clq都不可能被另外一个孩子代替，否则会得到更优的解。区间排个序然后扫一遍不就行了，搞这么麻烦干嘛
Maximum Clique Size of a set of Rectangles问题：与Intervals 不同在于给定是S个二维的矩阵而非S个一维的区间。
解法是对于每个矩阵的下边线，考虑与其相交的所有矩阵，对这些矩阵的左右边线调用Intervals算法。为了优化复杂度，类似Intervals的算法，建立左右边界的线段树。扫描下边界不断地插入删除左右区间即可。依然区间排序然后扫描时区间修改也行啊
Range Trees 问题：给定k维的区间R，搜索在R里所有的k维点。在一维情况下就是二分搜索左边界。
k维range tree的每个节点是一颗k-1维的range tree，建立的过程就是对于当前的根，取所有点最后一维的中位数，分成两个子树继续建立k维range tree，同时对于当前所有点建立前k-1维的range tree，挂在根上。用数学归纳法可得时空复杂度皆为$O(n\log^{k-1}n),\forall k\ge 2$。
查询的过程是找到一个节点s它的左右区间都包含当前维度的区间，这时s的左孩子可以不断的往右探直至它的左孩子有点在当前维度的区间，此时可知它的右孩子的最后一维都在给定的区间（显然比区间左值大，又s的右孩子有比区间右值小的值），则对右孩子进行k-1维range tree查找，最终到一维情况下二分查找，而对左孩子继续上述过程，直至叶子节点，直接检验。s的右孩子类似的处理。这个查询过程在当前k维range tree上最多查询$O(\log n)$个节点，这些节点都在k-1维的range tree的查询，所以若不考虑最终查找结果数，时间复杂度为$O(\log^k n)$。
可以进一步减少一个log：在2维时，此时每个点都按第一维排序，根下面挂的所有点都记录它在左右子树中第一个不比它小的点的位置，这样直接在根上查找比不必左端点小的元素，在左右子树中就不需要二分查找而是直接用根记录的信息传递下来即可。但是为了一个log增加一倍空间真的值吗？
Priority Search Trees 问题：给定n个形如$(x,y)$的二维点，要求查找一个点p，使得他在$x\in[l,r]$的所有点q中$y$最小。
所谓的Priority Search Tree基本上就是线段树的思想。不同在于它是从上往下建立的。它应用的另外一个Grounded 2D-Range Search Problem也可以直接用线段树解决。
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter11/">Chapter11</a>
  </h1>
  <time datetime="2019-07-02T19:08:12&#43;0800" class="post-date">Tue, Jul 2, 2019</time>
  Balanced binary search trees 直接跳到11.3节了，平衡树平衡策略分为三种：子树高度的限制，子树大小的限制，多路搜索树的二进制化。
重新平衡的思路基本上都是从叶子开始往根修复平衡，修复的方法是rotation，它将一个子树的高度减1，另一个加1。大多数的算法逆着路径上执行一次单旋或双旋。另外一种重新平衡的思路是重建某一颗子树。
有一些BST的重平衡时间低于log，这在一些场合比较有效，例如计算几何中有些情况下rotation复杂度不是O(1)。以及并发的重平衡。
AVL Tree AVL树是1962年发明的远古数据结构，它基于子树的高度进行平衡，采用rotation重平衡。AVL树要求对于任意点，它的子树高度差最多为1。直接对高度做induction，可得高度为h的AVL树至少有$F_{h+2}-1$个点，即深度是$O(\log n)$。重平衡是逆着路径进行单旋，或者双旋，或者调整子树信息。插入时最多进行一次旋转。在AVL树中如果只插入或者只删除，那么均摊时间是$O(1)$，否则是$O(\log n)$。
Weight-balanced tree Weight-balanced树基于子树的weight进行平衡，weight定义为叶子节点数。它要求每个点的weight与它右孩子的weight的比值在$[\alpha,1-\alpha]$之间。每上升一层，weight至少为原来的$\frac{1}{1-\alpha}$，这样得到深度和weight之间的关系。重平衡也是单旋或双旋，不同的是重平衡完成后再次重平衡至少需要当前weight的常数倍次的更新，这就导致了所谓的均摊时间少于$O(\log n)$。
Balanced Binary Trees Based on Multi-Way Trees binary B-trees类似trie的二进制实现那样，把一个节点拆成多个节点实现成二叉树。Symmetric Binary Trees或者通常被称为红黑树是2,3,4树的二进制实现。AA trees与红黑树等价，但是实现起来很容易。完全不讲细节的
看到11.5
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter16/">Chapter16</a>
  </h1>
  <time datetime="2019-06-24T22:21:33&#43;0800" class="post-date">Mon, Jun 24, 2019</time>
  B Trees B树是一种平衡树，每个节点存放一个磁盘block的内容，有多个孩子，所以是多叉树。它的要求是每个节点至少半满，而B*树要求每个节点至少2/3满。B树的思路导向了SOTA的spatial index结构：R树。
disk-based环境主要考察的是读写磁盘block的次数。尽管真实系统中有内存作为磁盘的buffer，但是在通常的分析中，这并不在研究的范围中。
B Tree B树的目的是在disk-based的环境下，给定一组key-value的object，进行高效的查询和更新（插入、删除）。这章依然假定不会有重复的key。B树的每个节点x有一个x.num记录x中保存的object数，第i个object用x.key[i],x.value[i]表示；叶子节点在同一层；非叶子节点x还保存x.num+1个孩子节点的地址，第i个子树是插在(x.key[i-1],x.key[i])之间的元素；非根节点是至少半满的，即若非叶子节点能存2B个孩子，则它至少存B个孩子；根节点至少存两个孩子。比如12章里的linked (2,4) Tree，不加link就是B树。下图是一个B=2的树。
查询：只需要扫面x.key数组找到待查询元素或者递归查询它所在的子树即可。
插入：设插入(k,v)，若根x满，这时有x.num=2B-1，取x.key的中间点作为新的根，把剩下的key各分配到新建的左孩子和右孩子，则它们都有B-1个key，即有B个孩子，所以依然是B树。这样就可以保证当前节点不满，递归调用notFull函数：如果是叶子，由于已知它不满，则可以插入，如果不是叶子，那么可以通过key数组找到包含它的一个子树，若子节点已满，将第B个key挪到当前节点（由于已知它不满），右边B-1个key挪到新建的节点，原理和处理根满类似，这样可以进入对应的不满的子节点，递归notFull。
删除：情况非常的复杂，需要在查询的过程中动态调整：
对于下面来说，可以保证只要不是根，当前节点就是非半满的。
1.若是叶子，直接删除。
2.非叶子且不包含k，若对应的节点半满，若它的邻居不半满，则进行类似rotate的操作，若它的邻居半满，则将合并。这步保证下一个孩子必然非半满。另外若当前节点是根，且根只有两个孩子，则深度减1。
3.非叶子且包含k，若这个key的左孩子或右孩子非半满，将k换成k‘并递归删除k’（k‘是最接近k的可删值，不妨设左孩子非半满，保证了下一步孩子非半满，则k’必然是从左孩子开始一直往右节点走的叶子的最右的key，所以不需要提前知道k&rsquo;，显然删除过程只会发生1,2两种情况），否则两个孩子都半满，将k和左右孩子合并，在子树中继续删除k（若当前节点是根，且根只有两个孩子，则深度减1）。
由于B是常数，所以以上过程在每个block上都是$O(1)$的操作，三个操作都最多访问$O(\log_B n)$个block。
层数增加只会发生在根节点，这样所有的叶子层数都增加1，层数减少只会发生在根上，这样所有的叶子都减少1，所以所有叶子在同一层。因此插入删除都维护了B树的性质。
B+ Tree B+树与B树主要不同在于只在叶子节点有object，而非叶子节点只存key，且所有的叶子节点都是用递增的双向链表连接。操作的维护比B树容易很多。这样会稍微违反BST的性质，但是可以保证只有右子树有与根相同的元素。
查询：类似与B树的查询，直接沿着树找到叶子即可。它的一个优势在于支持区间查询，由于区间是顺序双向连接的，所以通过查询找到左值所在的块，接着顺序读取连接的块即可。
插入：先查找到叶子节点并记录下查找的路径，将(k,v)插入叶子，逆着路径考虑，若当前节点满了，则将其从中间拆成两个（叶子需要维护链表），若不是叶子，则将右孩子的最大值挪进父亲，否则将右孩子的最大值复制进父亲。
删除：先查找到叶子节点并记录下查找的路径，删除(k,v)，当叶子非根时逆着路径考虑，若当前节点少于半满，若它同父亲的邻居半满，将它们合并，此时父亲节点也被删除了一个（叶子也要维护链表），若不是叶子，则孩子加入被删除的节点，否则不加入；若当前节点多余半满，类似B树的旋转。
由于插入和删除都只在根上加节点，所以必然保证叶子的深度相同，导致树高为$O(\log_B n)$。插入和删除需要注意的是分裂或者拼接时叶子和非叶子时不同的，因为叶子的值不能被拿走，但是非叶子代表的是区间，不拿走的话会出现无意义的区间，破坏B+树的性质。
IO次数少：B+树在数据库中被广泛采用，一个4层高的100+树就可以存0.3billion个objects，如果把前两层平均133个block存入内存，这只需要2个block的io就可以找到目标节点。
快速构建：直接建立需要$O(n\log_B n)$次io。可以更快的进行构建，先将objects排序，需要$O(n/B\log_B n)$次io，然后从底层开始一层层的建树，这样只要$O(n/B)$次io，总的建立需要$O(n/B\log_B n)$次io。
聚合查询：类似于线段树的思想，区间保存聚合结果，最大、最小、求和、计数都可以保存，用$O(\log_B n)$个block查询。
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter14/">chapter14</a>
  </h1>
  <time datetime="2019-06-24T12:06:55&#43;0800" class="post-date">Mon, Jun 24, 2019</time>
  Randomized Dictionary Structures 引言 worst-case高效的数据结构太复杂并且写起来很麻烦，所以人们试图优化一串操作的总代价，而非单次操作的代价，这就是所谓的均摊复杂度。从随机的角度，目的是限制操作的期望并是的超出阈值的概率不大。
为了简化分析，本章仅考虑支持插入、删除、搜索操作的字典，字典集为有限整数，并且元素之间两两不相同。
本章讨论的随机算法是Las Vegas algorithm，即只要返回结果，结果就是对的。另外一种是Monte Carlo algorithm，运行时间固定，但是结果不一定对。
$1-O(\beta^{-n}),1-O(n^{-c})$都是high confidence bound，$1-O((\log n)^c)$是very good confidence bound，$1-O(\beta^{-\alpha})$是constant probability bound。对于本章，通常考虑运行时间与期望时间的bound。
然后是一堆概率论的基础知识。
Skip Lists skip list的思路是当将有序链表$S_0$中随机一些节点复制进入$S_1$，接着从$S_1$复制一些节点到$S_2$，直至顶层为空。即前一层的节点以某个概率$p$继续成为上一层的节点，并将这两个节点连接起来，同层的节点也要连接起来。所以每个节点是一个变长的数组，记录每层的内容。如下图所示，32是一个节点，它有6层，每层之间相互连接，这样方便往右下搜索。
直接计算每一数的高度大于l的概率，然后直接算期望得高度为$O(\log n)$，随便union bound一下即可证明W.H.P.
空间复杂度期望$O(n)$，W.H.P.就Chernoff bound一下n个随机变量层高得空间复杂度$O(n)$。
查找：从顶层开始，尽可能的往右走，保证当前节点的值不大于目标节点，走到不能走则前往下一层的同列节点。
插入：按照查找的过程查找位置，不同在于需要记录每层最后一个不大于目标节点的节点。然后随机生成这个节点的层数，将其连在查找在记录下的节点后。
删除：类似查找并尝试在每层中删除即可。注意可能产生的空行。
以上三个操作运行时间都是由查找时间决定的。我们逆向考虑查找的过程，从底层0到顶层r的过程中每次进入上层时这个值都是上层中最接近x的值，即每次都是以$p$的概率往上走，以$1-p$的概率往左走，这等价于r个几何随机变量$Y_i$，每个表示第$i$层往左走的步数。注意到第$r$层为空，所以总的搜索步数为$Y=r+\sum_{i=0}^{r-1}Y_i$。这个转化也太帅了！
令$H_r=\sum_{i=0}^{r-1}Y_i$，由$P[r &gt; 4\log n]&lt;\frac{1}{n^3}$，$P[H_r&gt;16\log n]&lt;\frac{1}{n^2}\ for\ n\ge 32$是先全概率公式展开，再用Chernoff bound，则$P[H_r&gt;16\log n\cup r &gt; 4\log n]&lt;\frac{1}{n^2}+\frac{1}{n^3}=O(\frac{1}{n^2})$，即$P[Y=O(\log n)]&gt;1-O(\frac{1}{n^2})$。对于每一个x，$P[Y\in O(\log n)]&gt;1-O(\frac{1}{n^2})$，所以n+1种的路径均为$O(\log n)$的概率至少为$1-O(\frac{1}{n})$，这就证明了三个操作W.H.P都是$O(\log n)$。
Randomized Binary Search Trees Randomly Built Binary Tree(RBBT)指只有随机插入，即给定序列的树，插入在不同间隙里的概率是相等的。这个假定完全不符合实际，并且删除操作会使得RBBT退化严重，高度达到$\sqrt n$。
RBST和treap不同，它不随机产生第二有限值，而是保存当前节点的子树点数，并维护这样的性质$P[size(L)=i|size(T)=n]=\frac{1}{n}$，即不同大小左子树的概率相等，相当于有序列表里每个元素等概率为根。
以下建议全看原论文，好神难啊QAQ
插入：当插入一个n个点树时，随机一个0..n的数，若不是n，进入相对于的子树继续插入，否则x应当为根，递归的产生$T&lt;,T&gt;$，不妨设x小于根，根r及其右孩子因设为$T&gt;$，递归调用$r.L,x,T&lt;,T&gt;.L$，这是指$T&lt;$需要继续添加，$T&gt;$已经填好了根和右子树，只需要填左子树。
删除：先正常的搜索到这个节点，然后将这个节点的左子树和右子树join起来，join是判断随机产生的数[1,size(T)-1]是在左子树还是右子树，若在左子树，则将左孩子的右子树与右子树join，否则将右孩子的左子树与左子树join。
插入的复杂度与普通插入一样，用递归证明产生的两个子树也是RBST，同时$T&gt;.R$也是RBST，则证明$T&gt;$是RBST只需要证明根也满足条件，注意这个根是原树的根，所以用条件概率即可的证明。
现在考虑插入的结果，若未插入在根，则第一次会使概率变为$\frac{1}{n+1}$，递归得到一个RBST，原树剩下的部分也是RBST。否则用上面的结论另外注意x也是$\frac{1}{n+1}$的概率。
对于join，递归证明join后的子树是RBST，左孩子的左子树是RBST，右子树更小的join结果，也是RBST，互相独立，每个元素为根的概率是$\frac{1}{n+m}$。
  
  <div class="read-more-link">
    <a href="cartershi.github.io/blog/post/hdsa_chapter14/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter13/">Chapter13</a>
  </h1>
  <time datetime="2019-06-24T00:15:52&#43;0800" class="post-date">Mon, Jun 24, 2019</time>
   Splay trees 对于m个操作的序列，splay运行时间为$O(m\log n)$，所以均摊时间为$O(n)$。可以用来处理linking cutting trees，并且rebalance操作比平衡树简单很多。又是tarjan
复杂度分析 zig、zag是单旋（最多发生一次），zigzig、zagzag是父亲及父亲的父亲在同侧，zigzag、zagzig是父亲及父亲的父亲在异侧，直至将x转到根，这个操作称为splay(x)，这个操作均摊复杂度为$O(log n)$。
w(x)是x的权重，size(x)是以x为根的子树的点权重和，rank(x)是$\log_2(size(x))$，Potential(T)=$\alpha\sum_{x\in T}rand(x)$。均摊时间=实际时间+新Potential-老Potential。splay的运行时间为$\beta\times$Number of rotations。其中$w(x)\ge 0,\ \alpha\ge\beta&gt;0$为任意给定值。
将splay的旋转按上面分的三种旋转进行逐个分析，累加得，将对根为t的树进行splay(x)的均摊时间为$3\alpha(rank(t)-rank(x))+\beta=O(\log\frac{size(t)}{size(x)})$。
令$w(x)=\frac{1}{n}$，$\alpha=\beta=1$，则单次splay的均摊时间为$3\log n+1$，Potential的总变化最多为$O(n\log n)$，则进行m次splay操作的均摊时间是$O(3m\log n + m)$，得实际时间为$O(3m\log n + m)+n\log n=O((m+n)\log n)$。
以下这几个操作的定义与12章treap中的定义完全一致。
查找：按照BST查找，若成功找到，则对找到的点进行一次splay，否则对发现失败的点的父亲进行splay。
插入：用BST的查找找到叶子节点，未找到则对叶子splay，否则插在这个叶子的子树上，然后对其splay（这里与论文中的插入不同）。
合并：在t1里查找正无穷，则最大的点变成了t1的根，让t2变成t1的右子树。
分裂：调用查找x的过程，判断根与x的大小关系分开根和它的左或右子树。
删除：调用查找的过程，若找到，则因为查找最后调用了splay，所以根为待删除的节点，将根的左右子树进行合并即可。
书中的分析缺少这是势能定义，还是按论文里来。对于m个操作的序列，定义Potential所有树的Potential的和操作前不在树中的数的w和，然后对每种操作都类似splay可以分析出一个均摊的时间（基本上是splay的均摊时间+不同操作导致的势能变化），Potential的总变化最多为$O(n\log n)$，将它们累加得总时间为$O((m+n)\log n)$。
静态最优 上面的分析只是基于$w(x)=\frac{1}{n}$得出的，若定义$q(i)$为i被查询的次数（直接或间接），定义$w(x)=\frac{q(i)}{\sum q(i)}$，得splay均摊时间$-3rank(x)+1=O(1+\log\frac{\sum q(x)}{q(x)})$，求和得$O(\sum q(x)+\sum q(x)\log\frac{\sum q(x)}{q(x)})$，Potential最大变化为$O(\sum\log\frac{\sum q(x)}{q(x)})$，由于$m=\sum q(x)$，所以总时间为$O(m+\sum q(x)\log\frac{m}{q(x)})$，这是二叉搜索树的最优结果。类似的思路可以处理finger search，可证明它比finger search tree更好。
working set property：若只有t个元素被查询，查询的时间是$O(\log t)$，若$i$的两次访问之间间隔了$t$个不同的数，则i的第二次查询时间为$O(\log(t+1))$，这里主要用到了一个权重重排的思想，首先给定一个权重，第i个首次出现元素权重设为$\frac{1}{i^2}$，当某个元素被查询时，将它的权重变为1，比他大的权重都相应的向后挪一位（相当于循环调整），这样的重拍只会Potential变小，最终时间=均摊时间+老Potential-新Potential，得类似上面的分析算出的时间只会比真实之间更大。这也太狠了
附一篇文章Alternatives to splay trees with O(log n) worst-case access times,John Iacono
splay由于w函数可以具体设定，所以它被推测可能是动态最优的，即对于任意的序列，它都可以和最好的动态二叉搜索树一样。以被证明splay树有类似finger search的性质，搜索接近的目标很快。
LCT 
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/hdsa_chapter12/">Chapter12</a>
  </h1>
  <time datetime="2019-06-22T19:23:11&#43;0800" class="post-date">Sat, Jun 22, 2019</time>
  Finger Search Tree Finger Search指从finger $x$出发寻找$y$。比如有序数组，从$x$开始倍增，这里不能acm里的那种倍减trick，因为那样是$O(\log n)$的，而倍增是$O(\log d)$的。$d$是$x$和$y$在有序数组中的排名差。如下图所示，意识流一下就是那样了。
Dynamic Finger Search Trees 上面的倍增已经解决了这个问题，但是有序数组不能高效插入删除，所以需要Dynamic Finger Search Tree。
Level Linked (2,4)-Trees B树它leile，2-4个孩子的B树，基于finger的插入删除均摊复杂度为$O(1)$，期望倒是会，均摊不会分析。它与B树的不同在于所有的边都是双向的，并且同层相邻节点之间边相连。
不妨令$x&lt;y$，走到父亲$v$，同时判断$v$的右邻居$v&rsquo;$是否是$y$的父亲。当停止时：
若因为停在共同的祖先$c$，则$c$至少有一个子树隔开$x,y$，这颗子树里的点都在$x,y$之间，它的点数就bound住了高度。
若是因为$v$的右邻居$u&rsquo;$是$y$的父亲，此时$x$在$v$的右子树，$y$在$v$的父亲的右邻居$u&rsquo;$的左子树，只需要考虑$v$和$u&rsquo;$的子树的末端相邻点，则变成第一种情况。Q.E.D.
综上，用(2,4)-Tree的时间复杂度是$O(\log d)$的。
Randomized Finger Search Trees treap细节完全不介绍的吗？背景知识参考这里。
treap treap的思路就是一个二叉搜索树，但是它不像平衡树那样根据树高左旋右旋，而是根据随机产生的第二优先值维护一个堆（下面只考虑小根堆）。比如当前子树根为A，左子树根为B，右子树为C，不妨B的第二优先值最小，则B右旋成根，它的右子树变成A的左子树。单旋谁不会啊
下图字母是键值，数字是第二优先值。
插入：首先按二叉搜索树将其真实值插入到叶子，然后就堆插入元素那样按第二优先值从最底层往上调，调整就是用的左旋右旋。
删除：先找到这个节点，然后将其旋转到叶子，最后删除掉，逆着插入的操作
分裂：即分裂成比某个值$\pi$大的和小的两个treap。插入$\pi$，令它的第二优先值为无穷小，那么插入完它为根，且由二叉搜索树得，它的两个子树是要求的treap。
合并：用$\pi$作为根合并两棵treap，然后将根用删除的操作删除。
以上4个操作的复杂度均由treap的高度决定，可证明随机情况下高度为$O(\log n)$。
引理：将元素按键值递增为数组A，在treap中第i小的元素为第k小元素的祖先等价于i的第二优先值是A[i..k]中最小的(i&lt;k)。在treap中第i小的元素为第j小元素的祖先等价于i的第二优先值是A[j..i]中最小的(j&lt;i)
证：
⇒：考虑当前treap的根是谁。若是i，则得证；若是k，则矛盾；若在区间(i,k)，则i和k不在同一子树，矛盾；若不在区间[i,k]，则是更小的treap，递归得证。
⇐：依然分类讨论根。Q.E.D.
这个引理在下面的证明中会被直接使用，就不明说了。
定理1：以上四个操作期望复杂度都是$O(\log n)$的。
证：考虑每个元素深度的期望，即祖先个数的期望，即每个点是其祖先的概率，等价于每个区间中它是最小的，将区间拆成以它结尾的和以它开头的分开计算，两个都是$\sum\frac{1}{i}=O(\log n)$级别。所以以上操作都是$O(\log n)$的。
定理2：证明树高$O(\log d)$是W.H.P.的。
证：W.H.P.通常很多是用chernoff bound来证明的，类似树高的期望证明，将区间拆成两个，每个补充到n个随机变量，每个直接用chernoff bound即可。Q.E.D.
接下来分析基本来自96年Raimund Seidel的那篇Randomized Search Trees。或者算法导论红黑树那章最后一道作业题。
定理3：给定finger下，插入删除复杂度为$O(1)$。
证：定义点v的left spine为从v开始只走左边的最长路径，right spine为从v开始只走右边的最长路径。
对于待删除的点x，定义$SL(x)$为x的左孩子的right spine，$SR(x)$为x的右孩子的left spine。虽然很别扭，但是这个定义很符合单旋的性质：若x左旋，即x变成左孩子，x的右孩子的左子树变为x的右子树，则$SL(x)$不变，$SR(x)-1$。若右旋，即x变为右孩子，x的左孩子的右子树变为x的左子树，则$SL(x)-1$，$SR(x)$不变，所以每次旋转$SL(x)+SR(L)$减少一。左、右两个字我已经不认识了。
注意到$SL(x)$显然是$x$到$x-1$的路径（这里所有的$x$都是指A数组中第$x$个元素），若$x-1$不在$x$的子树中，则$x$没有左孩子了，否则$x-1$一定在right spine的终点。所以期望意义下$SL(x)$为A[1&hellip;x-1]中所有$x-1$的祖先减去$x$的祖先，即$SL(x)=\sum_{i=1}^{x-1}\frac{1}{(x-i+1)(x-i)}=1-\frac{1}{x}$。同理$SR(x)=1-\frac{1}{n+1-x}$。
也就是说给定finger的情况下，$O(1)$时间可以删除它，由于插入为删除的逆过程，给定finger时也是$O(1)$时间插入。Q.E.D.
定理4：接下来证明$x$到$y$的路径长度期望是$O(\log d)$。
  
  <div class="read-more-link">
    <a href="cartershi.github.io/blog/post/hdsa_chapter12/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/post/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%89%8B%E5%86%8C/">数据结构手册</a>
  </h1>
  <time datetime="2019-06-22T17:07:02&#43;0800" class="post-date">Sat, Jun 22, 2019</time>
  打算学一些数据结构上的分析，参考书Handbook of Data Structures and Applications，简称HSDA吧，在这里，由于书写的比较简略，有些地方不容易理解，所以做一些读书笔记。不能作为教程，主要写得是自己的体验。
另外附一个18年出版的第二版，不过排版没有第一版舒服。网上不太好找，放个压缩包，密码是nju。
还是按18年的目录记录吧。
如有侵权，请联系删除。
PARTI、PARTII太简单了，不做笔记了。
  
</article><article class="post">
  <h1 class="post-title">
    <a href="cartershi.github.io/blog/about/">About</a>
  </h1>
  <time datetime="2019-06-22T15:34:51&#43;0800" class="post-date">Sat, Jun 22, 2019</time>
  研究笔记
邮箱：wangyisyx@163.com
  
</article>
</div>
    </main>

    
  </body>
</html>
